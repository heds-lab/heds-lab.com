---
title: "Research"
type: "page"
---

## Combining human judgment and data-driven approaches for the development of interpretable models of student behaviors: Applications to computer science education (2020–2024)

{{< figure src="/images/research/CAREER-fig.png" class="research-image">}}

This NSF-funded project formalizes a hybridized approach to student modeling that generates models that are both interpretable by non-experts and robust to the noisiness of human behavior data. It combines knowledge elicitation approaches—which use human experts to develop interpretable models and to filter out noisy information—with data-driven approaches using machine learning algorithms which are often able to discover relationships that are difficult for human experts to explicate. We apply this hybridized approach in the context of computer science education, leveraging the increased interpretability of the models it creates to (1) better understand student debugging behaviors during programming activities, (2) support students in self-reflecting about the strategies they use and develop more efficient debugging strategies, (3) provide instructors with actionable information about their students’ debugging activities and (4) support future computer science teacher in acquiring expertise related to formulating hypotheses about a student’s debugging strategies.

[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1942962)


## Exploring algorithmic transparency: Interpretability and explainability of educational models

{{< figure src="/images/research/FFnetwork.png" class="research-image">}}

The HEDS lab is investigating the challenge of interpretability that comes with many of the state-of-the-art predictive models in education. We are pursuing three general threads of research: (1) understanding the differences and tradeoffs between using post-hoc explainability vs. intrinsically interpretable methods for transparency, (2) creating intrinsically interpretable machine learning models that retain the advantages of more complex algorithms, and (3) devising metrics and designs to effectively evaluate the interpretability of prediction models. We are interested in collaborating in this space, so feel free to reach out!

**Relevant links:**

- [Human-Centric eXplainable AI in Education Workshop (HEXED)](https://hexed-workshop.github.io), co-organized by members of our lab
- Liu, Q., Pinto, J. D., & Paquette, L. (2024). [Applications of explainable AI (XAI) in education](https://link.springer.com/chapter/10.1007/978-3-031-64487-0_5)
- Pinto, J. D., & Paquette, L. (2024). [Towards a unified framework for evaluating explanations](https://ceur-ws.org/Vol-3840/HEXED24_paper4.pdf)


### Constraints-based approach to interpretable-by-design modeling

As part of our efforts to explore algorithmic transparency, one approach we are taking is focused creating a fully interpretable model, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. This project involves three main goals: (1) develop an interpretable neural network, comparing accuracy and issues relevant to interpretability approaches as a whole, (2) evaluate this model's level of interpretability using a human-grounded evaluation approach, and (3) validate the model's inner representations and explore some hypothetical advantages of interpretable models, including their use for knowledge discovery. We are now working on our second goal and will post updated results on this page as we obtain them.

**Relevant links:**

- Pinto, J. D., Paquette, L., & Bosch, N. (2024). [Intrinsically interpretable artificial neural networks for learner modeling](https://educationaldatamining.org/edm2024/proceedings/2024.EDM-doctoral-consortium.121/)
- Pinto, J. D., Paquette, L., & Bosch, N. (2023). [Interpretable neural networks vs. expert-defined models for learner
behavior detection](https://jdpinto.com/papers/Pinto%20et%20al%202023_Interpretable%20neural%20networks%20vs.pdf)


## [INVITE AI Institute (2023-2028)](https://invite.illinois.edu)

{{< figure src="/images/research/INVITE-logo.png" class="research-image">}}

The HEDS is part of the INVITE Institute (INclusiVe Intelligent Technologies for Education), one of the NSF National AI Research Institutes. INVITE seeks to fundamentally reframe how AI-based educational technologies interact with learners by developing AI techniques to track and promote skills that underlie successful learning and contribute to academic success: persistence, academic resilience, and collaboration. The HEDS Lab primarily focuses on the learner modeling strand of the institute.

[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2229612&HistoricalAwards=false)


## [Using Data Mining and Observation to Derive an Enhanced Theory of SRL in Science Learning Environments (2016-2021)](https://wp0.vanderbilt.edu/oele/ecr-bettys-brain-bromp/)

{{< figure src="/images/research/BettysBrain.png" class="research-image">}}

The primary goal for this project was to enhance the theory and measurement of students’ self-regulated learning (SRL) processes during science learning by developing a technology based framework which leverages human expert judgment and machine learning methods to identify key moments during SRL and analyze these moments in depth. These issues are being studied in the context of Betty’s Brain, an open-ended science learning environment that combines learning-by-modeling with critical thinking and problem-solving skills to teach complex science topics. This project used data-driven detectors of student SRL behaviors and emotional states to conduct with in-the-moment qualitative interviews at key moments in the students' interaction with Betty's Brain.

[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1665216)


## [Improving Collaborative Learning in Engineering Classes Through Integrated Tools (2016-2021)](https://www.colearnlab.org/csteps2)

{{< figure src="/images/research/CSTEPS.jpg" class="research-image">}}

The goal of this project is to design and study tools that 1) support teaching assistants learning about and implementing collaborative learning and 2) support students engaged in collaborative problem solving activities. In this project, we used log-data produced by students as they work in groups on a tablet computer to develop automated detectors related to their collaboration. These detectors were used to drive automated in-the-moment prompts to guide instructors in supporting their students' collaborative problem solving.


[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1628976)
