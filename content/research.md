---
title: "Research"
type: "page"
---

## Combining human judgment and data-driven approaches for the development of interpretable models of student behaviors: Applications to computer science education (2020–2024)

{{< figure src="/images/research/CAREER-fig.png" class="research-image">}}

This NSF-funded project formalizes a hybridized approach to student modeling that generates models that are both interpretable by non-experts and robust to the noisiness of human behavior data. It combines knowledge elicitation approaches—which use human experts to develop interpretable models and to filter out noisy information—with data-driven approaches using machine learning algorithms which are often able to discover relationships that are difficult for human experts to explicate. We apply this hybridized approach in the context of computer science education, leveraging the increased interpretability of the models it creates to (1) better understand student debugging behaviors during programming activities, (2) support students in self-reflecting about the strategies they use and develop more efficient debugging strategies, (3) provide instructors with actionable information about their students’ debugging activities and (4) support future computer science teacher in acquiring expertise related to formulating hypotheses about a student’s debugging strategies.

[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1942962)


## Exploring algorithmic transparency: Interpretability and explainability of educational models

{{< figure src="/images/research/FFnetwork.png" class="research-image">}}

Our lab is investigating the challenge of interpretability that comes with many of the state-of-the-art predictive models in education. We are pursuing three general threads of research: (1) understanding the differences and tradeoffs between using post-hoc explainability vs. intrinsically interpretable methods for transparency, (2) creating intrinsically interpretable machine learning models that retain the advantages of more complex algorithms, and (3) devising metrics and designs to effectively evaluate the interpretability of prediction models. We are interested in collaborating in this space, so feel free to reach out!
