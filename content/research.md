---
title: "Research"
type: "page"
---

## Combining human judgment and data-driven approaches for the development of interpretable models of student behaviors: Applications to computer science education (2020–2024)

{{< figure src="/images/research/CAREER-fig.png" class="research-image">}}

This NSF-funded project formalizes a hybridized approach to student modeling that generates models that are both interpretable by non-experts and robust to the noisiness of human behavior data. It combines knowledge elicitation approaches—which use human experts to develop interpretable models and to filter out noisy information—with data-driven approaches using machine learning algorithms which are often able to discover relationships that are difficult for human experts to explicate. We apply this hybridized approach in the context of computer science education, leveraging the increased interpretability of the models it creates to (1) better understand student debugging behaviors during programming activities, (2) support students in self-reflecting about the strategies they use and develop more efficient debugging strategies, (3) provide instructors with actionable information about their students’ debugging activities and (4) support future computer science teacher in acquiring expertise related to formulating hypotheses about a student’s debugging strategies.

[NSF abstract](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1942962)


## Exploring algorithmic transparency: Interpretability and explainability of educational models

{{< figure src="/images/research/FFnetwork.png" class="research-image">}}

Our lab is investigating the challenge of interpretability that comes with many of the state-of-the-art predictive models in education. We are pursuing three general threads of research: (1) understanding the differences and tradeoffs between using post-hoc explainability vs. intrinsically interpretable methods for transparency, (2) creating intrinsically interpretable machine learning models that retain the advantages of more complex algorithms, and (3) devising metrics and designs to effectively evaluate the interpretability of prediction models. We are interested in collaborating in this space, so feel free to reach out!


## [INVITE AI Institute](https://invite.illinois.edu)

{{< figure src="/images/research/INVITE-logo.png" class="research-image">}}

Our lab is part of the INVITE Institute (INclusiVe Intelligent Technologies for Education), one of the NSF National AI Research Institutes. INVITE seeks to fundamentally reframe how AI-based educational technologies interact with learners by developing AI techniques to track and promote skills that underlie successful learning and contribute to academic success: persistence, academic resilience, and collaboration. The HEDS Lab primarily focuses on the learner modeling strand of the institute.


## [Betty's Brain](https://wp0.vanderbilt.edu/oele/ecr-bettys-brain-bromp/)

{{< figure src="/images/research/BettysBrain.png" class="research-image">}}

https://wp0.vanderbilt.edu/oele/ecr-bettys-brain-bromp/


## [CSTEPS](https://www.colearnlab.org/csteps2)

{{< figure src="/images/research/CSTEPS.jpg" class="research-image">}}

CSTEPS 2 (Collaborative Support Tools for Engineering Problem Solving 2)
